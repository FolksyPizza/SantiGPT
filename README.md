# <img src="https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/chatgpt.png" alt="GPT Logo" width="28" height="28"/> SantiGPT

<p align="left">
  <img src="https://logo.svgcdn.com/logos/python.svg" alt="Python Logo" height="100px;"/> <img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="Torch Logo" height="100"/>
</p>

SantiGPT is a high performance transformer (Self-Attention Network), built on the GPT-2 Architecture, with support of a context window of up to 1k tokens. SantiGPT has a variety of Domain Experts/Specialized Sub-Models.

# Capibilities of SantiGPT
- [x] Text Generation
- [x] Context-aware responses
- [x] Built on the GPT-2 Architecture, with up to a 1k token context window
- [x] Routes input to Domain Experts if necessary
## Created with:
- PyTorch
- Hugging Face Transformers
- GPT-2 Architecture

### Dataset On Hugging Face:
